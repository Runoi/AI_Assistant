import hashlib
import os
from pathlib import Path
import requests
import csv
from io import StringIO
from datetime import datetime
from typing import List, Optional, Set
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
import asyncio
from urllib.parse import urlparse
from dotenv import load_dotenv

load_dotenv('.env')

OPENAI_API_KEY = str(os.getenv("OPENAI_API_KEY"))


class ChromaVectorStore:
    def __init__(self, persist_directory: str = "chroma_data"):
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY,
            openai_proxy=os.getenv("OPENAI_PROXY"),
        )
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )
    
    async def aadd_documents(self, documents: List[Document]) -> int:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ ChromaDB"""
        if documents:
            await self.vectorstore.aadd_documents(documents)
            return len(documents)
        return 0
    
    async def get_existing_hashes(self) -> Set[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ö–µ—à–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        existing = self.vectorstore._collection.get(include=["metadatas"])
        if existing and "metadatas" in existing:
            return {
                meta["content_hash"]
                for meta in existing["metadatas"]
                if "content_hash" in meta
            }
        return set()

class GoogleSheetsParser:
    def __init__(self, vectorstore: ChromaVectorStore):
        self.vectorstore = vectorstore
        self.processed_hashes: Set[str] = set()

    async def initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ö–µ—à–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.processed_hashes = await self.vectorstore.get_existing_hashes()
        print(f"üîç –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.processed_hashes)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ö–µ—à–µ–π")

    @staticmethod
    def _get_export_url(sheet_url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ CSV"""
        try:
            if '/spreadsheets/d/' in sheet_url:
                sheet_id = sheet_url.split('/spreadsheets/d/')[1].split('/')[0]
            else:
                sheet_id = sheet_url.split('/d/')[1].split('/')[0]
            return f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv"
        except IndexError:
            raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL Google –¢–∞–±–ª–∏—Ü—ã")

    def _generate_hash(self, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç MD5 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    async def parse_sheet(self, sheet_url: str) -> List[Document]:
        """–ü–∞—Ä—Å–∏—Ç Google –¢–∞–±–ª–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        try:
            export_url = self._get_export_url(sheet_url)
            print(f"üîó –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ URL: {export_url}")
            
            response = requests.get(export_url, timeout=30)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            return self._process_csv(response.text)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return []

    def _process_csv(self, csv_content: str) -> List[Document]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç CSV –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        documents = []
        current_answer = ""
        
        reader = csv.reader(StringIO(csv_content))
        next(reader)  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        
        for row in reader:
            if len(row) < 2:
                continue
                
            question = row[0].strip()
            answer = row[1].strip() or current_answer
            
            if not question:
                continue
                
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö–µ—à
            content = f"{question}{answer}"
            content_hash = self._generate_hash(content)
            
            if content_hash not in self.processed_hashes:
                documents.append(Document(
                    page_content=question,
                    metadata={
                        "answer": answer,
                        "source": "google_sheets",
                        "content_hash": content_hash
                    }
                ))
                self.processed_hashes.add(content_hash)
            
            if row[1].strip():
                current_answer = row[1].strip()
        
        return documents
    
    async def parse_prompt_from_sheet(self, sheet_url: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ –ª–∏—Å—Ç–∞ 'Prompt' –≤ Google Sheets"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID —Ç–∞–±–ª–∏—Ü—ã –∏–∑ URL
            sheet_id = sheet_url.split('/d/')[1].split('/')[0]
            
            # URL –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –ª–∏—Å—Ç–∞ 'Prompt' –≤ CSV
            prompt_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet=Prompt"
            
            response = requests.get(prompt_url, timeout=30)
            response.raise_for_status()
            
            # –ß–∏—Ç–∞–µ–º CSV, –∏—â–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ–ø—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
            reader = csv.reader(StringIO(response.text))
            for row in reader:
                if row and row[0].strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
                    return row[0].strip()
                    
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ–º–ø—Ç–∞: {e}")
            return None
