import hashlib
import os
from pathlib import Path
import requests
import csv
from io import StringIO
from datetime import datetime
from typing import List, Optional, Set
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
import asyncio
from urllib.parse import urlparse
from dotenv import load_dotenv

load_dotenv('.env')

OPENAI_API_KEY = str(os.getenv("OPENAI_API_KEY"))


class ChromaVectorStore:
    def __init__(self, persist_directory: str = "chroma_data"):
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY,
            openai_proxy=os.getenv("OPENAI_PROXY"),
        )
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )
    
    async def aadd_documents(self, documents: List[Document]) -> int:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ ChromaDB"""
        if documents:
            await self.vectorstore.aadd_documents(documents)
            return len(documents)
        return 0
    
    async def get_existing_hashes(self) -> Set[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ö–µ—à–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        existing = self.vectorstore._collection.get(include=["metadatas"])
        if existing and "metadatas" in existing:
            return {
                meta["content_hash"]
                for meta in existing["metadatas"]
                if "content_hash" in meta
            }
        return set()
    
    async def get_existing_questions(self) -> Set[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        existing = self.vectorstore._collection.get(include=["documents"])
        if existing and "documents" in existing:
            return {
                self._normalize_question(doc)
                for doc in existing["documents"]
            }
        return set()
    def _normalize_question(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        return text.strip().lower().replace("?", "").replace("!", "").replace(".", "")


class GoogleSheetsParser:
    def __init__(self, vectorstore: ChromaVectorStore):
        self.vectorstore = vectorstore
        self.processed_hashes: Set[str] = set()
        self.existing_questions: Set[str] = set()  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é

    
    async def initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.processed_hashes = await self.vectorstore.get_existing_hashes()
        self.existing_questions = await self.vectorstore.get_existing_questions()  # –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è
        print(f"üîç –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.existing_questions)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")

    @staticmethod
    def _get_export_url(sheet_url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ CSV"""
        try:
            if '/spreadsheets/d/' in sheet_url:
                sheet_id = sheet_url.split('/spreadsheets/d/')[1].split('/')[0]
            else:
                sheet_id = sheet_url.split('/d/')[1].split('/')[0]
            return f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv"
        except IndexError:
            raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL Google –¢–∞–±–ª–∏—Ü—ã")

    def _generate_hash(self, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç MD5 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    async def parse_sheet(self, sheet_url: str) -> List[Document]:
        """–ü–∞—Ä—Å–∏—Ç Google –¢–∞–±–ª–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        try:
            export_url = self._get_export_url(sheet_url)
            print(f"üîó –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ URL: {export_url}")
            
            response = requests.get(export_url, timeout=30)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            all_documents = self._process_csv(response.text)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –≤–æ–ø—Ä–æ—Å–∞
            new_documents = []
            duplicate_count = 0
            
            for doc in all_documents:
                norm_question = self._normalize_question(doc.page_content)
                if norm_question in self.existing_questions:
                    duplicate_count += 1
                    continue
                    
                new_documents.append(doc)
                self.existing_questions.add(norm_question)
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_documents)} –∑–∞–ø–∏—Å–µ–π, –∏–∑ –Ω–∏—Ö –Ω–æ–≤—ã—Ö: {len(new_documents)}, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_count}")
            return new_documents
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return []

    def _process_csv(self, csv_content: str) -> List[Document]:
        seen = set()
        documents = []
        last_valid_answer = None  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–µ–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
        
        reader = csv.reader(StringIO(csv_content))
        headers = [h.strip().lower() for h in next(reader)]
        
        try:
            q_col = headers.index("–≤–æ–ø—Ä–æ—Å")
            a_col = headers.index("–æ—Ç–≤–µ—Ç")
        except ValueError:
            return []
        
        for row in reader:
            if len(row) <= max(q_col, a_col):
                continue
                
            question = self._normalize_question(row[q_col])
            answer = self._normalize_answer(row[a_col])
            
            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞–ª–∏–¥–Ω—ã–π
            if not answer and last_valid_answer:
                answer = last_valid_answer
            elif answer:  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
                last_valid_answer = answer
                
            if not question or not answer:
                continue
                
            content_key = hashlib.md5(f"{question.lower()}{answer.lower()}".encode()).hexdigest()
            if content_key in seen:
                continue
                
            seen.add(content_key)
            documents.append(Document(
                page_content=question,
                metadata={
                    "answer": answer,
                    "source": "google_sheets",
                    "content_hash": content_key
                }
            ))
        
        return documents

    def _normalize_question(self, text: str) -> str:
        """–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É"""
        if not isinstance(text, str):
            return ""
        return text.strip().capitalize()

    def _normalize_answer(self, text: str) -> str:
        """–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É"""
        if not isinstance(text, str):
            return ""
        return text.strip()
        
    async def parse_prompt_from_sheet(self, sheet_url: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ –ª–∏—Å—Ç–∞ 'Prompt' –≤ Google Sheets"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º ID —Ç–∞–±–ª–∏—Ü—ã –∏–∑ URL
            sheet_id = sheet_url.split('/d/')[1].split('/')[0]
            
            # URL –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –ª–∏—Å—Ç–∞ 'Prompt' –≤ CSV
            prompt_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet=Prompt"
            
            response = requests.get(prompt_url, timeout=30)
            response.raise_for_status()
            
            # –ß–∏—Ç–∞–µ–º CSV, –∏—â–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ–ø—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
            reader = csv.reader(StringIO(response.text))
            for row in reader:
                if row and row[0].strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É
                    return row[0].strip()
                    
            return None
        except Exception as e:
            
            return None
